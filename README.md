

---

### Table of Contents

* **AI Domains and Tasks: A Comprehensive Study Guide**
    * **I. Overview of AI Domains**
    * **II. Language AI Domain**
        * A. Language-Related AI Tasks
        * B. Text as Data
        * C. Language AI Models
    * **III. Audio & Speech AI Domain**
        * A. Speech-Related AI Tasks
        * B. Audio/Speech as Data
        * C. Audio and Speech AI Models
    * **IV. Vision AI Domain**
        * A. Vision-Related AI Tasks
        * B. Images as Data
        * C. Vision AI Models
    * **V. Other AI Tasks**
* **Quiz: AI Domains and Tasks**
* **Quiz Answer Key**
* **Essay Format Questions**
* **Glossary of Key Terms**
# AI Domains and Tasks: A Comprehensive Study Guide

This document provides a comprehensive overview of the primary domains and tasks in Artificial Intelligence, including Language, Audio & Speech, and Vision. It also covers other important AI tasks such as Anomaly Detection, Recommendations, and Forecasting.

## I. Overview of AI Domains

This section introduces the three primary domains of AI discussed in the source material: Language, Audio & Speech, and Vision. It also touches upon other AI tasks like Anomaly Detection, Recommendations, and Forecasting.

## II. Language AI Domain

This domain focuses on AI tasks related to processing, understanding, and generating natural language.

### A. Language-Related AI Tasks

* **Text-related Tasks:** Use text as input and produce varied outputs.
    * **Examples:** Language detection, entity extraction, key phrase extraction, text translation.
* **Generative AI Tasks:** Output text is generated by a model.
    * **Examples:** Creating stories or poems, summarizing text, answering questions (e.g., ChatGPT).
    * **Mechanism:** Trained on large language models, continuously grow through machine learning.

### B. Text as Data

* **Sequential Nature:** Text consists of sentences, which are sequential collections of words.
* **Tokenization:** Converting words into numbers for model training.
* **Padding:** Making all sentence lengths equal.
* **Similarity Measurement:**
    * **Methods:** Dot similarity, cosine similarity.
    * **Concept:** Similar words/sentences should be represented as "close by" through embedding.
* **Embedding:** A representation indicating proximity of similar words or sentences.

### C. Language AI Models

* **Definition:** AI models designed to understand, process, and generate natural language.
* **Training:** Trained on vast amounts of textual data.
* **Deep Learning Architectures:**
    * **Recurrent Neural Networks (RNNs):** Process data sequentially, store hidden states.
    * **Long Short-Term Memory (LSTM):** Process data sequentially, retain context better using gates.
    * **Transformers:** Process data in parallel, use self-attention for better context understanding.

## III. Audio & Speech AI Domain

This domain focuses on AI tasks related to processing, understanding, and generating audio and speech.

### A. Speech-Related AI Tasks

* **Audio/Speech as Input:**
    * **Examples:** Speech-to-text conversion, speaker recognition, voice conversion.
* **Generative AI Tasks:** Output audio is generated by a model.
    * **Examples:** Music composition, speech synthesis.

### B. Audio/Speech as Data

* **Digitization:** Audio is digitized as snapshots taken in time.
* **Sample Rate:** Number of times per second an audio sample is taken (e.g., 44.1 kHz for audio CDs).
    * **Process:** Audio sampled during recording and reconstructed during playback.
* **Bit Depth:** Number of bits in each sample, indicating information richness.
* **Correlation of Samples:** Multiple samples need to be correlated to infer meaning from audio data.

### C. Audio and Speech AI Models

* **Definition:** Designed to process and understand audio data, including spoken language.
* **Deep Learning Architectures (considering sequential nature of audio):**
    * Recurrent Neural Networks (RNNs)
    * Long Short-Term Memory (LSTM)
    * Transformers
    * Variational Autoencoders
    * Waveform Models
    * Siamese Networks

## IV. Vision AI Domain

This domain focuses on AI tasks related to processing, understanding, and generating images and visual data.

### A. Vision-Related AI Tasks

* **Image as Input:**
    * **Examples:** Classifying images, identifying objects in an image.
    * **Popular Application:** Facial recognition (surveillance, biometrics, law enforcement, social media).
* **Generative AI Tasks:** Output image is generated by a model.
    * **Examples:** Creating images from contextual descriptions, generating specific styles or high-resolution images.
    * **Capabilities:** Creating realistic new images/videos, generating original 3D models (objects, machine components, buildings, people).

### B. Images as Data

* **Composition:** Images consist of pixels (grayscale or color).
* **Information Inference:** Cannot infer much from a single pixel; context from multiple pixels is required.
* **Task Dependence:** The AI task determines the type of input needed and output produced.

### C. Vision AI Models

* **Deep Learning Architectures:**
    * **Convolutional Neural Networks (CNNs):** Detects patterns in images, learns hierarchical representations of visual features.
    * **YOLO (You Only Look Once):** Processes images to detect objects efficiently.
    * **Generative Adversarial Networks (GANs):** Generates real-looking images.

## V. Other AI Tasks

Beyond the three main domains, the source material mentions additional AI applications.

* **Anomaly Detection:**
    * **Data Type:** Time series data (single or multivariate).
    * **Applications:** Fraud detection, machine failure.
* **Recommendations:**
    * **Data Type:** Data of similar products or users.
    * **Application:** Recommending products.
* **Forecasting:**
    * **Data Type:** Time series data.
    * **Applications:** Weather forecasting, predicting stock prices.

---

## Quiz: AI Domains and Tasks

* **Instructions:** Answer each question in 2-3 sentences.
1.  What is the primary difference between "text-related tasks" and "generative AI tasks" within the language domain?
2.  Explain the purpose of "tokenization" and "padding" in the context of preparing text data for language models.
3.  Name two deep learning model architectures commonly used for language tasks and briefly describe one key characteristic of each.
4.  How is audio data "digitized," and what role does "sample rate" play in this process?
5.  Why is it difficult to infer meaning from a single audio sample, and what is required to make sense of audio data?
6.  Identify two deep learning architectures specifically mentioned for audio and speech AI models, highlighting a shared consideration among them.
7.  Give two examples of "image-related tasks" within the vision AI domain, and briefly describe the function of one of them.
8.  What is the main function of Generative Adversarial Networks (GANs) in the context of vision-related AI tasks?
9.  Describe how "time series data" is relevant to both "anomaly detection" and "forecasting."
10. How does the "task that needs to be performed" influence the input and output in AI, particularly in the vision domain?

---

## Quiz Answer Key

1.  **Text-related tasks** use existing text as input to perform operations like extraction or translation, where the output varies based on the task. **Generative AI tasks**, conversely, produce entirely new text as output, such as stories or summaries, generated by a model.
2.  **Tokenization** is the process of converting words into numerical representations, which is necessary for language models to process text. **Padding** involves adjusting sentence lengths to be uniform, ensuring consistent input dimensions for the models.
3.  **Recurrent Neural Networks (RNNs)** process data sequentially and store hidden states to maintain context. **Transformers** process data in parallel and use a mechanism called self-attention to better understand the context of the input.
4.  Audio data is **digitized** by taking "snapshots" of sound at specific points in time. The **sample rate** dictates how many of these snapshots are taken per second, determining the fidelity of the digital audio.
5.  A single audio sample provides insufficient information to infer much meaning because sound is a sequential phenomenon. Multiple samples need to be correlated over a duration to make sense of the data and recognize patterns.
6.  **Recurrent Neural Networks (RNNs)** and **Long Short-Term Memory (LSTM)** are two architectures used for audio and speech AI models. All these models are designed to take into consideration the inherently sequential nature of audio data.
7.  **Classifying images** involves categorizing an image based on its content (e.g., dog, cat). **Identifying objects in an image** focuses on locating and naming specific items within the visual scene (e.g., finding all cars in a street photo).
8.  The main function of **Generative Adversarial Networks (GANs)** in vision AI is to generate highly realistic-looking images. They do this by pitting two neural networks against each other: one generating images and the other trying to distinguish real from generated images.
9.  **Time series data** is crucial for **anomaly detection** because it allows the system to identify unusual patterns or deviations over time, like in fraud or machine failure. For **forecasting**, time series data is used to predict future values or trends based on historical sequences, such as weather or stock prices.
10. In the **vision domain**, the specific AI task (e.g., classifying an image vs. generating an image) directly dictates the type of input required (e.g., an existing image vs. a text description) and the nature of the output produced (e.g., a label vs. a new image).

---

## Essay Format Questions

1.  Compare and contrast the deep learning model architectures used across the Language, Audio & Speech, and Vision AI domains, identifying any commonalities and specialized architectures for each. Discuss why certain architectures are better suited for specific data types or tasks.
2.  Explain the concept of "generative AI" across the three domains (Language, Audio & Speech, Vision). Provide specific examples for each domain and discuss the transformative potential of generative AI.
3.  Discuss the process of preparing raw data (text, audio, images) for use in AI models. Detail the specific steps and concepts (e.g., tokenization, padding, sample rate, bit depth, pixels) involved in transforming this data into a format suitable for machine learning algorithms.
4.  Beyond the core three domains, the source mentions "Other AI tasks." Choose two of these tasks (Anomaly Detection, Recommendations, Forecasting) and describe their applications, the type of data they typically require, and how they contribute to real-world problem-solving.
5.  Analyze the significance of "similarity" (e.g., dot similarity, cosine similarity, embedding) in AI models, particularly within the language domain. How does the concept of representing similar data points as "close by" enhance the capabilities of language models?

---

## Glossary of Key Terms

* **Anomaly Detection:** An AI task that identifies unusual patterns or deviations in data, often time series, used for applications like fraud or machine failure.
* **Audio and Speech AI Models:** AI models specifically designed to process and understand audio data, including spoken language.
* **Bit Depth:** The number of bits in each sample of digital audio, indicating the richness of information in that sample.
* **Convolutional Neural Networks (CNNs):** A deep learning architecture primarily used for vision tasks, adept at detecting patterns and learning hierarchical representations of visual features in images.
* **Cosine Similarity:** A measure of similarity between two non-zero vectors of an inner product space that measures the cosine of the angle between them; often used to quantify the similarity between text or word embeddings.
* **Deep Learning Model Architectures:** Specific structural designs of neural networks used to train AI models, such as RNNs, LSTMs, Transformers, CNNs, etc.
* **Dot Similarity:** A method to measure the similarity between two vectors by taking their dot product, often used in machine learning contexts.
* **Embedding:** A representation of words or sentences as vectors in a multi-dimensional space, where similar words/sentences are represented as being "close by."
* **Entity Extraction:** A text-related AI task that identifies and extracts specific entities (e.g., names, locations, organizations) from unstructured text.
* **Facial Recognition:** A popular image-related AI task that identifies or verifies a person from a digital image or a video frame, often used in security and surveillance.
* **Forecasting:** An AI task that predicts future trends or values based on historical time series data, applied in areas like weather or stock prices.
* **Generative AI:** A type of AI that generates new content (text, audio, images) that did not exist before, based on its training data.
* **Generative Adversarial Networks (GANs):** A deep learning architecture used in vision tasks to generate realistic images by pitting two neural networks (a generator and a discriminator) against each other.
* **Image-related Tasks:** AI tasks that use an image as input and produce an output dependent on the task, such as classification or object identification.
* **Language AI Models:** Artificial intelligence models designed to understand, process, and generate natural language.
* **Long Short-Term Memory (LSTM):** A type of recurrent neural network architecture that processes data sequentially and can retain context better over long sequences through the use of "gates."
* **Machine Learning:** A subset of AI that enables systems to learn from data, identify patterns, and make decisions with minimal human intervention.
* **Natural Language Processing (NLP):** A field of AI that focuses on enabling computers to understand, interpret, and generate human language.
* **Padding:** The process of adding filler (e.g., zeros) to shorter sequences (like sentences) to make all sequences equal in length for model processing.
* **Pixels:** The basic building blocks of digital images, representing individual points of color or grayscale.
* **Recurrent Neural Networks (RNNs):** A type of deep learning architecture that processes sequential data and stores information in "hidden states," making them suitable for language and audio tasks.
* **Recommendations:** An AI task that suggests products or content to users based on data of similar products or users.
* **Sample Rate:** In digital audio, the number of times per second an audio sample is taken during recording and playback (measured in Hertz).
* **Self-attention:** A mechanism used in Transformers that allows the model to weigh the importance of different parts of the input sequence when processing a specific element, improving context understanding.
* **Speech Synthesis:** A generative AI task that creates artificial human speech from text.
* **Speech-to-text Conversion:** A speech-related AI task that converts spoken audio into written text.
* **Text-related Tasks:** AI tasks that use text as input, with outputs varying based on the task, such as language detection or entity extraction.
* **Time Series Data:** A sequence of data points indexed (or listed) in time order, crucial for tasks like anomaly detection and forecasting.
* **Tokenization:** The process of breaking down text into smaller units (tokens, often words or sub-word units) and converting them into numerical representations for AI model input.
* **Transformers:** A deep learning architecture that processes data in parallel and uses a self-attention mechanism, highly effective for various language and increasingly other sequential data tasks.
* **Variational Autoencoders:** A type of neural network used for generative tasks, including in audio and speech models.
* **Waveform Models:** AI models that work directly with the raw audio waveform, used in audio and speech processing.
* **YOLO (You Only Look Once):** A real-time object detection system in the vision domain that processes entire images and detects objects within them in a single pass.
